{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from random import randint\n",
    "import numpy\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_system(title,plot):\n",
    "    \n",
    "    movies=pd.read_csv(\"movies_metadata.csv\")\n",
    "    movies=movies.loc[:,['id','title','overview']]\n",
    "    movies.loc[len(movies)]=[randint(0,10)+1000,title,plot]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    movies['overview'] = movies['overview'].fillna('')\n",
    "    tfidf_matrix = tfidf.fit_transform(movies['overview'])\n",
    "    similarity_distance = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
    "    \n",
    "    def get_recommendations(title, cosine_sim=similarity_distance):\n",
    "        # Get the index of the movie that matches the title\n",
    "        idx  = movies['id'][movies['title']==title].index[0]\n",
    "\n",
    "        # Get all movies with same similarity score\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "        # Sort the movies based on the similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the movie indices\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "        # Return the top 10 most similar movies\n",
    "        return movies.iloc[movie_indices][:10]\n",
    "\n",
    "    data=get_recommendations(title)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_guide(link):\n",
    "    ua = UserAgent()\n",
    "    response=requests.get(link, {\"User-Agent\": ua.random})\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    seasons=0\n",
    "\n",
    "    for i in soup.select('#title-episode-widget > div > div:nth-child(4)'):\n",
    "        k=i.find_all('a')\n",
    "        if(k):\n",
    "            for j in k[:1]:\n",
    "                seasons=int(j.text.strip())\n",
    "        else:\n",
    "            seasons=0\n",
    "\n",
    "    if(seasons):\n",
    "        for i in range(seasons):\n",
    "            #link='https://www.imdb.com/title/tt0903747/'\n",
    "            link_=link+'episodes?season={}'.format(i)\n",
    "            response=requests.get(link_, {\"User-Agent\": ua.random})\n",
    "            soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "            content=soup.find('div',class_='clear')\n",
    "            season_no=content.find('h3',id='episode_top')\n",
    "            print(season_no.text.strip()+'\\n')\n",
    "\n",
    "            content2=content.find('div',class_='list detail eplist')\n",
    "\n",
    "            for j in content2.find_all('div',class_='list_item odd'):\n",
    "                    title=j.find('strong')\n",
    "                    title_name=title.find('a')\n",
    "                    print('Episode Name: '+title_name['title']+\"\\t\")\n",
    "\n",
    "                    rating=j.find('div',class_='ipl-rating-widget')\n",
    "                    if(rating):\n",
    "                        rating_=rating.find('div',class_='ipl-rating-star small')\n",
    "                        if(rating_):\n",
    "                            rating_1=rating_.find('span',class_='ipl-rating-star__rating')\n",
    "                            if(rating_1):\n",
    "                                print('Rating: '+rating_1.text.strip()+'\\t')\n",
    "\n",
    "                    date=j.find('div',class_='airdate')\n",
    "                    if(date):\n",
    "                        print(\"Date Aired: \"+date.text.strip()+'\\t')\n",
    "\n",
    "                    summary=j.find('div',class_='item_description')\n",
    "                    if(summary):\n",
    "                        print('Summary: '+summary.text.strip()+\"\\n\")\n",
    "\n",
    "\n",
    "            for j in content2.find_all('div',class_='list_item even'):\n",
    "                    title=j.find('strong')\n",
    "                    if(title):\n",
    "                        title_name=title.find('a')\n",
    "                        print('Episode Name: '+title_name['title']+\"\\t\")\n",
    "\n",
    "                    rating=j.find('div',class_='ipl-rating-widget')\n",
    "                    if(rating):\n",
    "                        rating_=rating.find('div',class_='ipl-rating-star small')\n",
    "                        if(rating_):\n",
    "                            rating_1=rating_.find('span',class_='ipl-rating-star__rating')\n",
    "                            if(rating_1):\n",
    "                                print('Rating: '+rating_1.text.strip()+'\\t')\n",
    "\n",
    "                    date=j.find('div',class_='airdate')\n",
    "                    if(date):\n",
    "                        print(\"Date Aired: \"+date.text.strip()+'\\t')\n",
    "\n",
    "                    summary=j.find('div',class_='item_description')\n",
    "                    if(summary):\n",
    "                        print('Summary: '+summary.text.strip()+\"\\n\")\n",
    "\n",
    "    else:\n",
    "        print('No guide as it is a movie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract information \n",
    "def scrapper_media(imdb_link):\n",
    "        \n",
    "        ua = UserAgent()\n",
    "        response=requests.get(imdb_link, {\"User-Agent\": ua.random})\n",
    "        soup=BeautifulSoup(response.text,'lxml')\n",
    "        \n",
    "        if(re.search('https://www.imdb.com',imdb_link)):\n",
    "            \n",
    "            #Name\n",
    "            for j in soup.select('#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper')[:1]:\n",
    "                if(j):\n",
    "                    print('Name:'+j.text.strip()[:20]+\"\\n\\n\")\n",
    "                    title=j.text.strip()[:20]\n",
    "                    \n",
    "                    \n",
    "            #Storyline\n",
    "            for j in soup.select('#titleStoryLine div:nth-child(3) p span'):\n",
    "                if(j):\n",
    "                    print('Storyline:\\n'+j.text.strip()+\"\\n\\n\")\n",
    "                    plot=j.text.strip()\n",
    "                \n",
    "            #Rating\n",
    "            for k in soup.select('#title-overview-widget div.vital div.title_block div div.ratings_wrapper div.imdbRating div strong span'):\n",
    "                if(k):\n",
    "                    print(\"Rating:\"+k.text+\"\\n\\n\")\n",
    "                \n",
    "            #Number of Seasons\n",
    "            for i in soup.select('#title-episode-widget > div > div:nth-child(4)'):\n",
    "                k=i.find_all('a')\n",
    "                if(k):\n",
    "                    for j in k[:1]:\n",
    "                        print(\"Number of Seasons: \"+j.text.strip()+'\\n')\n",
    "                        \n",
    "                    \n",
    "            #Number of Episodes    \n",
    "            for k in soup.select('#title-overview-widget > div.vital > div.button_panel.navigation_panel > a > div > div > span'):\n",
    "                if(k):\n",
    "                    print(\"Number of episodes:\"+k.text.strip()[:3]+\"\\n\")\n",
    "                    \n",
    "                    \n",
    "            #Episode Length\n",
    "            for i in soup.select('#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper > div > time'):\n",
    "                if(i):\n",
    "                    print(\"Episode Length:\"+i.text.strip()+\"\\n\\n\")\n",
    "                \n",
    "            #Poster    \n",
    "            content=soup.find('div',class_='poster')\n",
    "            if(content):\n",
    "                co=content.find('a')\n",
    "                co2=co.find('img')\n",
    "                response = requests.get(co2['src'])\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                plt.imshow(img)\n",
    "                \n",
    "            #Genres        \n",
    "            for k in soup.select('#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper > div'):\n",
    "                if(k):\n",
    "                    q=k.find_all('a')\n",
    "                    print(\"Genres:\\n\")\n",
    "                    for i in q[:-1]:\n",
    "                        print(i.text)\n",
    "                    print('\\n')\n",
    "                \n",
    "            #Creators and Star Cast    \n",
    "            content=soup.find('div',class_='plot_summary')\n",
    "            if(content):\n",
    "                for i in content.find_all('div',class_='credit_summary_item'):\n",
    "                    j=i.find('h4')\n",
    "                    if(j):\n",
    "                        if((j.text.strip()=='Creator:')|(j.text.strip()=='Creators:')):\n",
    "                            w=i.find_all('a')\n",
    "                            print('Creator:')\n",
    "                            for s in w:\n",
    "                                if(s.text.strip()=='See full cast & crew'):\n",
    "                                    break\n",
    "                                else:\n",
    "                                    print(s.text.strip())\n",
    "                            print('\\n')\n",
    "                        elif(j.text.strip()=='Stars:'):\n",
    "                            w=i.find_all('a')\n",
    "                            print('Stars:')\n",
    "                            for s in w:\n",
    "                                if(s.text.strip()=='See full cast & crew'):\n",
    "                                    break\n",
    "                                else:\n",
    "                                    print(s.text.strip())\n",
    "                            print('\\n')\n",
    "                    else:\n",
    "                        print('Not Available')\n",
    "                \n",
    "            \n",
    "            #Release Data\n",
    "            for i in soup.select('#titleDetails'):\n",
    "                k=i.find_all('div',class_='txt-block')\n",
    "                if(k):\n",
    "                    for j in k[3:4]:\n",
    "                        print(j.text.strip()[:40])\n",
    "                    print('\\n')\n",
    "                \n",
    "            #Trivia And Goofs    \n",
    "            content=soup.find('div',id='titleDidYouKnow')\n",
    "            if(content):\n",
    "                trivia=content.find('div',id='trivia')\n",
    "                if(trivia):\n",
    "                    print(\"Trivia: \"+trivia.text[7:-24].strip()+\"\\n\")\n",
    "                    \n",
    "                goof=content.find('div',id=\"goofs\")\n",
    "                if(goof):\n",
    "                    print('Goofs: '+goof.text[6:-24].strip()+\"\\n\")\n",
    "            \n",
    "    \n",
    "            #People also like    \n",
    "            content3=soup.find('div',class_='rec_slide')\n",
    "            if(content3):\n",
    "                print('People also like: \\n')\n",
    "                for i in content3.find_all('div',class_='rec_item'):\n",
    "                    j=i.find('a')\n",
    "                    k=j.find('img')\n",
    "                    print(k['alt'])\n",
    "                print('\\n\\n')\n",
    "            \n",
    "            \n",
    "            #Reviews    \n",
    "            reviews=[]\n",
    "            link2=imdb_link+'reviews?ref_=tt_urv'\n",
    "            response2=requests.get(link2, {\"User-Agent\": ua.random})\n",
    "            soup2=BeautifulSoup(response2.text,'lxml')\n",
    "            content2=soup2.find('div',class_='lister-list')\n",
    "            \n",
    "            ctr=0\n",
    "            \n",
    "            print('User Reviews: \\n')\n",
    "            if(content2):\n",
    "                for i in content2.find_all('div',class_='review-container'):\n",
    "                    j=i.find('div',class_='content')\n",
    "                    print(j.text.strip())\n",
    "                    print('-----------------------------------------------------------------------------------------------------------')\n",
    "                    ctr=ctr+1\n",
    "                    reviews.append(j.text.strip())\n",
    "\n",
    "                    if(ctr==5):\n",
    "                        break\n",
    "            \n",
    "            #Photos\n",
    "            q=[]\n",
    "            for i in soup.select('#titleImageStrip > div.mediastrip'):\n",
    "                k=i.find_all('a')\n",
    "                if(k):\n",
    "                    for j in k:\n",
    "                        a=j.find('img')\n",
    "                        print(a['loadlate'])\n",
    "                        q.append(a['loadlate'])\n",
    "\n",
    "                else:\n",
    "                    print('Not Available')\n",
    "                    \n",
    "            \n",
    "            type_='show'\n",
    "            \n",
    "            for i in soup.select('#titleDetails > h3:nth-child(11)'):\n",
    "                x=i.text\n",
    "                \n",
    "                if x:\n",
    "                    if x=='Box Office':\n",
    "                        \n",
    "                        type_='movie'                        \n",
    "                        c=input('Do you want to show recommended movies')\n",
    "                        if(c=='y' or c=='yes'):\n",
    "                            df=recommendation_system(title,plot)\n",
    "                            print(df)\n",
    "                        else:\n",
    "                            print('Bye')\n",
    "                    else:\n",
    "                        c=input('Want the episode guide (y/n)')\n",
    "        \n",
    "                        if(c=='y'or c=='yes'):\n",
    "                            episode_guide(imdb_link)\n",
    "                        else:\n",
    "                            print('Bye')\n",
    "                else:\n",
    "                    c=input('Want the episode guide (y/n)')\n",
    "                    \n",
    "                    if(c=='y'or c=='yes'):\n",
    "                        episode_guide(imdb_link)\n",
    "                    else:\n",
    "                        print('Bye')\n",
    "                        \n",
    "            if(type_=='show'):\n",
    "                c=input('Want the episode guide (y/n)')\n",
    "                if(c=='y'or c=='yes'):\n",
    "                    episode_guide(imdb_link)\n",
    "                else:\n",
    "                    print('Bye')\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            print('Not Available')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapper_star(imdb_link):\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    imdb_links=imdb_link[:-1]\n",
    "    imdb_links2=imdb_links+'bio?ref_=nm_ov_bio_sm'\n",
    "    response=requests.get(imdb_links2, {\"User-Agent\": ua.random})\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    response2=requests.get(imdb_links, {\"User-Agent\": ua.random})\n",
    "    soup2=BeautifulSoup(response2.text,'lxml')\n",
    "    \n",
    "    if(re.search('https://www.imdb.com',imdb_links)):\n",
    "\n",
    "        #Name and picture\n",
    "        content=soup.find('div',class_='subpage_title_block name-subpage-header-block')\n",
    "        if(content):\n",
    "            a=content.find('a')\n",
    "            im=a.find('img')\n",
    "            k=im['src']\n",
    "            response = requests.get(k)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            plt.imshow(img)\n",
    "                    \n",
    "        #Title   \n",
    "        title=content.find('div',class_='parent')\n",
    "        if(title):\n",
    "            print(\"Name: \"+title.text.strip()+'\\n')\n",
    "            name=title.text.strip()\n",
    "          \n",
    "\n",
    "        #Overview\n",
    "        content2=soup.find('table',class_='dataTable labelValueTable')\n",
    "        if(content2):\n",
    "            for i in content2.find_all('tr'):\n",
    "                #print(\":\")\n",
    "                j=i.text.strip()\n",
    "                j=j.replace('\\n',\" \").replace('    ',' ')\n",
    "                print(j)\n",
    "            print('\\n\\n')\n",
    "            \n",
    "\n",
    "        #Bio\n",
    "        content3=soup.find('div',class_='soda odd')\n",
    "        p=content3.find('p')\n",
    "        if(p):\n",
    "            print(p.text.strip())\n",
    "            print('\\n\\n')\n",
    "        \n",
    "        #Spouse\n",
    "        content4=soup.find('table',id='tableSpouses')\n",
    "        if(content4):\n",
    "\n",
    "            for i in content4.find_all('tr'):\n",
    "                j=i.find('td')\n",
    "                print(\"Spouse: \"+j.text.strip()+\"\\n\\n\")\n",
    "\n",
    "        #Known For\n",
    "        content5=soup2.find('div',id='knownfor')\n",
    "        if(content5):\n",
    "            print('Known For: ')\n",
    "            for i in content5.find_all('div',class_='knownfor-title'):\n",
    "                j=i.find('div',class_='knownfor-title-role')\n",
    "                k=j.find('a')\n",
    "                print(k['title'])\n",
    "            print('\\n\\n')\n",
    "\n",
    "\n",
    "        #Awards Won\n",
    "        link=imdb_links+'awards?ref_=nm_awd'\n",
    "        response=requests.get(link, {\"User-Agent\": ua.random})\n",
    "        soup4=BeautifulSoup(response.text,'lxml')\n",
    "        content7=soup4.find('div',class_='article listo')\n",
    "        if(content7):\n",
    "            q=[]\n",
    "            for i in content7.find_all('table',class_='awards'):\n",
    "                a=i.find_all('tr')\n",
    "                for j in a:\n",
    "                    k=j.find_all('td')\n",
    "                    row=[w.text for w in k]\n",
    "                    q.append(row)\n",
    "            df=pd.DataFrame(q)\n",
    "            #print(q)\n",
    "            df[0]=df[0].str.replace('\\n','')\n",
    "            df[1]=df[1].str.replace('\\n',' ')\n",
    "            df[2]=df[2].str.replace('\\n',' ')\n",
    "            df=df[df[1].str.contains('Winner',na=False)]\n",
    "            df=df.reset_index()\n",
    "            df.drop('index',axis=1,inplace=True)\n",
    "            df.columns=['Year','Award',\"Category\"]\n",
    "            df['Award']=df['Award'].apply(lambda x: x[7:])\n",
    "            #print(df)\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "                print(df)\n",
    "                \n",
    "            \n",
    "            return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_extract(name):\n",
    "    query = name\n",
    "    \n",
    "    query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
    "    number_result = 1\n",
    "    ua = UserAgent()\n",
    "    google_url = \"https://www.google.com/search?q=\" + query +\"imdb\"+\"&num=\" + str(number_result)\n",
    "    response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
    "    for r in result_div:\n",
    "        # Checks if each element is present, else, raise exception\n",
    "        try:\n",
    "            link = r.find('a', href = True)\n",
    "            title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
    "            #description = r.find('div', attrs={'class':'s3v9rd'}).get_text()\n",
    "            # Check to make sure everything is present before appending\n",
    "            if link != '':\n",
    "                imdb_links=link['href'][7:44]\n",
    "                #print(imdb_links)\n",
    "                #titles.append(title)\n",
    "                #descriptions.append(description)\n",
    "                # Next loop if one element is not present\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    return imdb_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(category):\n",
    "    \n",
    "        \n",
    "    \n",
    "    if(category=='movie' or category=='show'):\n",
    "        name=input('Please specify the name of movie/show:')\n",
    "        imdb_link=link_extract(name)\n",
    "\n",
    "        scrapper_media(imdb_link)\n",
    "\n",
    "    elif(category=='star'):\n",
    "                \n",
    "        name=input('Please specify the name of star:')\n",
    "        imdb_link=link_extract(name)\n",
    "        scrapper_star(imdb_link)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('wrong input')\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    import tflearn\n",
    "    import tensorflow\n",
    "      \n",
    "    \n",
    "    with open('intents2.json') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "    \n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])\n",
    "            \n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "    \n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "        \n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "    \n",
    "    tensorflow.reset_default_graph()\n",
    "\n",
    "    net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "    net = tflearn.regression(net)\n",
    "\n",
    "    model = tflearn.DNN(net)\n",
    "    \n",
    "    #model.fit(training, output, n_epoch=3000, batch_size=8, show_metric=True)\n",
    "    #model.save(\"model.tflearn\")\n",
    "    \n",
    "    model.load(\"model.tflearn\")\n",
    "    \n",
    "    def bag_of_words(s, words):\n",
    "        bag = [0 for _ in range(len(words))]\n",
    "\n",
    "        s_words = nltk.word_tokenize(s)\n",
    "        s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "        for se in s_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == se:\n",
    "                    bag[i] = 1\n",
    "\n",
    "        return numpy.array(bag)\n",
    "    \n",
    "    def chat():\n",
    "        print(\"Start talking with the bot (type quit to stop)!\")\n",
    "        while True:\n",
    "            inp = input(\"You: \")\n",
    "            if inp.lower() == \"quit\":\n",
    "                break\n",
    "            \n",
    "\n",
    "            results = model.predict([bag_of_words(inp, words)])\n",
    "            results_index = numpy.argmax(results)\n",
    "            tag = labels[results_index]\n",
    "\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg['tag'] == tag:\n",
    "                    responses = tg['responses']\n",
    "                    \n",
    "            \n",
    "            print(random.choice(responses))\n",
    "            \n",
    "            if(random.choice(responses)=='Please Wait'):\n",
    "            \n",
    "                if(tag=='MediaMovie'):\n",
    "                    ask('movie')\n",
    "                elif(tag=='Mediashow'):\n",
    "                    ask('show')\n",
    "                elif(tag=='Mediastar'):\n",
    "                    ask('star')\n",
    "\n",
    "            \n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\layers\\core.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\objectives.py:70: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\layers\\estimator.py:189: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:571: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:115: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:164: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:165: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:166: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bansa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tflearn\\helpers\\trainer.py:167: The name tf.get_collection_ref is deprecated. Please use tf.compat.v1.get_collection_ref instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\bansa\\Media-Scrapper-With-Recommendation-System-And-Chatbot-master\\Media-Scrapper-With-Recommendation-System-And-Chatbot-master\\model.tflearn\n",
      "Start talking with the bot (type quit to stop)!\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
